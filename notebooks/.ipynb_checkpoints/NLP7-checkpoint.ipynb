{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed96256f-676c-4ba5-883b-af285db7adef",
   "metadata": {},
   "source": [
    "# Importul librariilor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc62c25-974b-4b42-b2e1-299f6656612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2a582-6dff-4a18-8f34-81bab2deb3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5181989-fa33-4af2-a597-876f265d6293",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b425f-65fb-4cdb-b51f-456fb62de8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functie de mapare\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b326a0-8402-429f-94e1-2ea4ddc7f2ab",
   "metadata": {},
   "source": [
    "# Citirea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff488e6-eb90-4089-82c1-6c5f7de2123e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/berinde/curs-analiza-datelor-complexe/blob/main/data/input/1.input_data.csv?raw=True'\n",
    "reviews = pd.read_csv(url)\n",
    "print(reviews.shape)\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a0ebc-12a0-4047-b5a1-80e95bfb1b53",
   "metadata": {},
   "source": [
    "# Prepararea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b53c5-a4b2-4b30-9770-767e53d4512d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformarea din float in int a coloanei rating\n",
    "reviews['rating'] = reviews['rating'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6cbd1-cc60-46e6-aa80-1e5ba2e868a2",
   "metadata": {},
   "source": [
    "## Tokenizare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3164c-3f37-4a5d-b7d2-d1398de335bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50  # padding senquence in 50 tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# initialize two arrays for input tensors\n",
    "Xids = np.zeros((len(reviews), SEQ_LEN))\n",
    "Xmask = np.zeros((len(reviews), SEQ_LEN))\n",
    "\n",
    "for i, sentence in enumerate(reviews['text']):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n",
    "    if i % 10000 == 0:\n",
    "        print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466f204-8009-4b74-bff7-6a6c7665bc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = reviews['rating'].values  # transformarea coloanei rating in array\n",
    "labels = np.zeros((arr.size, arr.max()+1))  \n",
    "labels[np.arange(arr.size), arr] = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57620ffa-98ae-4515-be7d-fc592c278d72",
   "metadata": {},
   "source": [
    "## Salvarea datelor tokenizate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f052d-3a7c-43fe-90e7-a99f716705c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('xids.npy', 'wb') as f:\n",
    "    np.save(f, Xids)\n",
    "with open('xmask.npy', 'wb') as f:\n",
    "    np.save(f, Xmask)\n",
    "with open('labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b5003-f072-42ab-aa29-45a90931b23f",
   "metadata": {},
   "source": [
    "## Mapare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4d392-7ae5-4bbc-b1fe-6fb0dee9cd74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32  # we will use batches of 32\n",
    "\n",
    "# arrays dataset pentru tf\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "# folosind metoda de mapare mapam dataset-ul\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "# shuffle data\n",
    "dataset = dataset.shuffle(9500).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492208c-97cc-4f0c-a2e8-fa83c459ed9a",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd146d-0c59-4f44-819a-8e7c7b813e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DS_LEN = len([0 for batch in dataset])\n",
    "SPLIT = 0.8  # 80-20 split\n",
    "\n",
    "train = dataset.take(round(DS_LEN*SPLIT))  # primele 90% din batches\n",
    "val = dataset.skip(round(DS_LEN*SPLIT))  # sarim peste primele 90% si pastram doar 10%\n",
    "\n",
    "del dataset  # optional stergerea dataset-ului pentru mai multa memorie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1b984-4a3b-44ef-9e89-a3847bbb1a46",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89331a2c-a7e8-4ea2-9d12-27bd8f9afbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializare model Bert\n",
    "# bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# embeddings\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "X = tf.keras.layers.LSTM(64)(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "# y = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)\n",
    "y = tf.keras.layers.Dense(6, activation='softmax', name='outputs')(X)\n",
    "\n",
    "# definireal layerelor de input si output\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze the BERT layer - otherwise we will be training 100M+ parameters...\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b26c69-f01e-4185-b68f-d570ed2f4a89",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b8d22-7e32-4bb3-988c-6517e84da302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83fda2-98cb-4359-95c7-9af80d3874fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "history = model.fit(train, validation_data=val, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613f47a-93c2-48fe-bf82-06dcc1fb8e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "loss, accuracy = model.evaluate(val)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d5c63-dbb9-40bb-90a4-2ca08e9584a1",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215edd45-f9b0-41d2-9cee-3da0c1f03d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    'this is such an amazing meal!',  \n",
    "    'The meal was great!',\n",
    "    'The dinner was meh.',\n",
    "    'The lunch was okish.',\n",
    "    'The meal was terrible...'\n",
    "]\n",
    "\n",
    "label=[5,4,1,2,1]\n",
    "\n",
    "arr = np.array(label)\n",
    "label = np.zeros((arr.size, arr.max()+1))  \n",
    "label[np.arange(arr.size), arr] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8974e-2d08-4691-a322-010761fe2474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xids = np.zeros((len(examples), SEQ_LEN))\n",
    "Xmask = np.zeros((len(examples), SEQ_LEN))\n",
    "\n",
    "for i, sentence in enumerate(examples):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n",
    "    if i % 10000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c26e3-bce1-4d81-a8f0-bf06c99bf048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arrays dataset pentru tf\n",
    "dataset_pred = tf.data.Dataset.from_tensor_slices((Xids, Xmask, label))\n",
    "\n",
    "# folosind metoda de mapare mapam dataset-ul\n",
    "dataset_pred = dataset_pred.map(map_func)\n",
    "\n",
    "dataset_pred = dataset_pred.shuffle(9500).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ac312-9979-4e2f-957b-cb33f778552e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(dataset_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71093b3-fd93-4ec4-b124-5d07bc67ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "loss, accuracy = model.evaluate(dataset_pred)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000998a-af9e-4ae0-b87b-2cdc3a8fc37c",
   "metadata": {},
   "source": [
    "## Simpler example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344f39e-47fc-426f-a428-8657df74ee1d",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/text/tutorials/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7891f-47b4-4f51-ac79-656ead0b952e",
   "metadata": {},
   "source": [
    "## Transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c7076-5587-4faa-900a-092388976b7c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/renaudmathieu/transformer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba900d9-037e-4c45-bdf5-6d908ef7cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py310_xgb",
   "language": "python",
   "name": "nlp_py310_xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
